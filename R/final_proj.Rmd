---
title: "EDS 222: Final Project"
author: "Carly Caswell"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load all the packages needed here
library(tidyverse) #for tidying my data
library(readr)
library(gt)
library(tufte)
library(feasts)
library(dplyr) #for wrangling
library(dotwhisker) #for pretty plotting
library(plotly) #for plotting
library(lubridate) #for extracting specific years in the data
library(sf)
library(ggmap)
library(tmap)
library(here)
library(zoo) #moving averages

# Setting my filepath here:
rootdir <- ("~/Documents/MEDS/Fall_Q2/EDS-222-Stats/Assignments")
datadir <- file.path(rootdir,"eds222-final","data")
```

ABOUT:

I am interested in the trend of snowfall over time in Vermont and how the trend may be influencing the ski industry, Vermont's tourism economy, and potential wildlife and habitat implications that rely on snowfall to migrate and find food. Snowfall also largely helps water sources in the Spring, providing water to agriculture, wildlife, and for human consumption. I am specifically hoping to answer the question: are seasonal temperature differences causing snowfall patterns later in the year (ski resorts opening later or earlier) in Vermont? Are the more recent years of snowfall statistically significant given previous years of data?

I am also interested in the relationship between temperature changes and snowfall trends due to climate change and will be evaluating both temperature and snowfall variables in Vermont.

DATA:

I gathered daily snow depth and temperature data from the USDA's Natural Resources Conservation Service and National Water and Climate Center (<https://nwcc-apps.sc.egov.usda.gov/site-plots/#VT>). This data source contains two datasets:

Dataset 1 - Titled `vt_temps` , this dataset contains daily temperature data (in degrees Fahrenheit) from the years 2001 - 2023. These data points were collected from a weather station at the Mount Mansfield SCAN Site numbered 2041. Mount Mansfield is Vermont's largest mountain peak, located in the Green Mountains at an elevation of 4395'.

Dataset 2 - Titled `vt_snow` , this dataset contains daily snowfall data (in inches) from the years 2001 - 2023. These data points were collected from a weather station at the Mount Mansfield SCAN Site numbered 2041. Mount Mansfield is Vermont's largest mountain peak, located in the Green Mountains at an elevation of 4395'.

```{r}
# Reading in the data:
vt_temps <- read_csv(file.path(datadir,"Mount_Mansfield_temps.csv"))    
vt_snow <- read_csv(file.path(datadir,"Mount_Mansfield_snow.csv"))

```

EXPLORATION & WRANGLING:

Let's look at the temperature data first:

```{r}
#Let's take a look at some of this data further:
head(vt_temps)

#Making this data tidy:
tidy_vt_temps <- vt_temps %>%
  pivot_longer(cols = c('2001', '2002', '2003', '2004', '2005', '2006', '2007', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2021', '2022', '2023'), names_to = "year", values_to = "value") %>% 
  select(-'2009', -'2010', -'2019', -'2024') 

#creating a combined date column
tidy_vt_temps$date <- paste(tidy_vt_temps$year, tidy_vt_temps$date, sep = "-")
class(tidy_vt_temps$date) #it's still a character so this needs to be updated
tidy_vt_temps$date <- ymd(tidy_vt_temps$date)
class(tidy_vt_temps$date) #nice, it's now a date column

#Plotting a few years of data:
ggplot(tidy_vt_temps, aes(x = date, y = value)) +
  geom_line()

#I've decided I want to compare months November - December 
#Filter for only the months I want:
tidy_vt_temps_winter <- tidy_vt_temps %>% 
   mutate(month = month(date)) %>% 
  rename(temp = value)


#Calculating the mean and standard dev for the temps for each month each year:
temp_monthly_stats = tidy_vt_temps_winter %>% 
  group_by(year, month(date)) %>% 
  summarize("Monthly_Mean" = round(mean(temp, na.rm = TRUE), 2), "Monthly_SD" = round(sd(temp, na.rm = TRUE), 2))

#Reviewing the results table:
print(temp_monthly_stats)
#The year with the highest mean and variance in number of cases is 2022.
```

Now looking at the snowfall data:

```{r}
#Let's take a look at some of this data further:
head(vt_snow)

#Making this data tidy:
tidy_vt_snow <- vt_snow %>%
  pivot_longer(cols = c('2001', '2002', '2003', '2004', '2005', '2006', '2007', '2011', '2012', '2013', '2015', '2016', '2017', '2018', '2022', '2023'), names_to = "year", values_to = "value") %>% 
  select(-'2009', -'2010', -'2020', -'2024') 

#creating a combined date column
tidy_vt_snow$date <- paste(tidy_vt_snow$year, tidy_vt_snow$date, sep = "-")
class(tidy_vt_snow$date) #it's still a character so this needs to be updated
tidy_vt_snow$date <- ymd(tidy_vt_snow$date)
class(tidy_vt_snow$date) #nice, it's now a date column

#Plotting a few years of data:
ggplot(tidy_vt_snow, aes(x = date, y = value)) +
  geom_line()

#I've decided I want to compare months November - December
#Filter for only the months I want:
tidy_vt_snow_winter <- tidy_vt_snow %>% 
  mutate(month = month(tidy_vt_snow$date)) %>% 
  rename(snowfall = value)

#Calculating the mean and standard dev for the snowfall in inches for each month each year:
snow_monthly_stats = tidy_vt_snow_winter %>% 
  group_by(year, month(date)) %>% 
  summarize("Monthly_Mean" = round(mean(snowfall, na.rm = TRUE), 2), "Monthly_SD" = round(sd(snowfall, na.rm = TRUE), 2))

#Reviewing the results table:
print(snow_monthly_stats)
#The year with the highest mean and variance in number of cases is 2022.
print(max(snow_monthly_stats$Monthly_Mean, na.rm = TRUE))
```

```{r}
#Just getting the months I want to compare:

tidy_vt_snow_winter_years <- 
tidy_vt_snow_winter %>% 
   filter(month %in% c(11,12)) %>% 
  mutate(month_day = sprintf("%02d-%02d", month(date), day(date)))

tidy_vt_temp_winter_years <-
tidy_vt_temps_winter %>% 
  filter(month %in% c(11,12)) %>% 
  mutate(month_day = sprintf("%02d-%02d", month(date), day(date)))
```

#ANALYSIS

Now that I have all my data, I can plot it together to get a visual comparison of years:


```{r}
#graph depicting annual snowfall....
plot_ly(tidy_vt_snow_winter_years, 
        x = ~month_day, 
        y = ~snowfall, 
        color = ~as.factor(year), 
        type = "scatter",
        mode = "lines+markers",
        line = list(width = 1)) %>%
  layout(title = "Comparison of Snowfall on Mount Mansfield from 2001 to 2022",
         xaxis = list(title = "Dates"),
         yaxis = list(title = "Snowfall (in inches)"),
         legend = list(title = "Year"))
```

```{r}
#graph depicting annual temps...
plot_ly(tidy_vt_temp_winter_years, 
        x = ~month_day, 
        y = ~temp, 
        color = ~as.factor(year), 
        type = "scatter",
        mode = "lines+markers",
        line = list(width = 1)) %>%
  layout(title = "Comparison of Temperature Recorded on Mount Mansfield from 2001 to 2022",
         xaxis = list(title = "Dates"),
         yaxis = list(title = "Temperature (in Fahrenheit)"),
         legend = list(title = "Year"))
```

SUMMARY STATISTICS:

```{r}
# I'm going to join the two datasets together first:
vt_data <- merge(tidy_vt_snow_winter, tidy_vt_temps_winter, by = 'date')

#taking a look at the data:
vt_data <- vt_data %>% 
  select(-"Min.x", -"10%.x",-"30%.x", -"70%.x", -"90%.x", -"Max.x", -"Median (POR).x", -"year.x", -"month.x", -"Min.y",-"10%.y", -"30%.y",-"70%.y",-"90%.y",-"Max.y",-"Median (POR).y") %>% 
  rename(year = "year.y", month = "month.y") %>% 
  filter( year %in% c('2001', '2002', '2003', '2015', '2016','2022'))
vt_data

#Summarizing the snowfall and temperature data for recent years:
vt_recent <- 
  vt_data %>% 
  filter(year %in% c('2015', '2016', '2022'))

vt_recent_stats <- vt_recent %>% 
summarise(snow_mean = mean(snowfall, na.rm = TRUE), snow_sd = sd(snowfall, na.rm = TRUE), snow_median = median(snowfall, na.rm = TRUE),temp_mean = mean(temp, na.rm = TRUE), temp_sd = sd(temp, na.rm = TRUE), temp_median = median(temp, na.rm = TRUE))

#visualizing the stats table:
vt_recent_stats

#Summarizing the snowfall and temperature data before for non-recent years:
vt_non_recent <- 
  vt_data %>% 
  filter(year %in% c('2001', '2002', '2003'))

vt_non_recent_stats <- vt_non_recent %>% 
summarise(snow_mean = mean(snowfall, na.rm = TRUE), snow_sd = sd(snowfall, na.rm = TRUE), snow_median = median(snowfall, na.rm = TRUE),temp_mean = mean(temp, na.rm = TRUE), temp_sd = sd(temp, na.rm = TRUE), temp_median = median(temp, na.rm = TRUE))

#visualizing the stats table:
vt_non_recent_stats
```

Now I'm going to look at the temperature and snowfall by splitting my data. Since I have years of data from 2001 - 2022, with a few years missing data, I want to bucket more recent years to years farther in the past. For this example, I'm going to bucket my data to look at the years 2015,2016,2022 (recent) and 2001, 2002, and 2003 (non-recent).

One big challenge here was the fact that we had quite limited data for many years, with some years not even being in the dataset. There was not information on the website about why there were years missing or there were many NAs, so I was limited to working with the years that did have complete data.

Snowfall Table:

| Years                         | Mean (inches) | Standard Deviation (inches) | Median (inches) |
|--------------------|------------------|------------------|------------------|
| Recent (2015,2016, 2022)      | 3.92          | 8.37                        | 0               |
| Non-Recent (2001, 2002, 2003) | 8.61          | 11.48                       | 0               |

Temperature Table:

| Years                         | Mean (F) | Standard Deviation (F) | Median (F) |
|--------------------|------------------|------------------|------------------|
| Recent (2015,2016, 2022)      | 42.28    | 18.83                  | 44.06      |
| Non-Recent (2001, 2002, 2003) | 39.64    | 19.43                  | 41.18      |


**OLS:**

I am aiming to draw conclusions about the Vermont snow climate using this sample from Mount Mansfield of temperature and snowfall observations. In order to draw conclusions, I need to check the OLS assumptions before running any regression. To do that I am going to plot the distribution of my snowfall and temperature samples.

```{r}
#Snowfall Distributions Check
#Non recent years
vt_hist_non <-
  ggplot(vt_non_recent) +
  geom_histogram(aes(x = snowfall)) +
labs(
    title = "Recent Snowfall Distribution During November and December",
    x = "Snowfall (inches)",
    y = "Frequency"
  )
vt_hist_non

#Recent years
vt_hist_recent <-
  ggplot(vt_non_recent) +
  geom_histogram(aes(x = snowfall)) +
labs(
    title = "Recent Snowfall Distribution During November and December",
    x = "Snowfall (inches)",
    y = "Frequency"
  )
vt_hist_recent

```

\*\*Both datasets are skewed to the right. There is a right long tail. This distribution is unimodal.\*\*

```{r}
#Temperature Distributions Check
#Non recent years
vt_hist_non <-
  ggplot(vt_non_recent) +
  geom_histogram(aes(x = temp)) +
labs(
    title = "Recent Temperature Distribution During November and December",
    x = "Degrees (Fahrenheit)",
    y = "Frequency"
  )
vt_hist_non

#Recent years
vt_hist_recent <-
  ggplot(vt_non_recent) +
  geom_histogram(aes(x = temp)) +
labs(
    title = "Recent Temperature Distribution During November and December",
    x = "Degrees (Fahrenheit)",
    y = "Frequency"
  )
vt_hist_recent
```

\*\*This data is faily symmetric. This distribution looks somewhat normally distributed. This distribution is bimodal.\*\*

I'm going to plot QQ plots to see how my normal distribution fits:

```{r}
##QQ Plot for Snowfall
ggplot(vt_recent, aes(sample = snowfall)) +
  geom_qq() +
  geom_qq_line() +
  ggtitle("Q-Q Plot for Snowfall")
```

```{r}
#QQ Plot for Temperature
ggplot(vt_non_recent, aes(sample = temp)) +
  geom_qq() +
  geom_qq_line() +
  ggtitle("Q-Q Plot for Temperature")
```

```{r}
##Transformed QQ Plot for Snowfall
log_snowfall <-log(vt_recent$snowfall)
log_snowfall_non <- log(vt_non_recent$snowfall)

ggplot(vt_recent, aes(sample = log_snowfall)) +
  geom_qq() +
  geom_qq_line() +
  ggtitle("Q-Q Plot for Transformed Precipitation Data")
```

There is a tighter concentration around the line which suggests there is now a closer fit to the normal distribution.

I can now run a hypothesis test to see if there is no difference in means from my recent sample group and my non-recent sample group.

HYPOTHESIS TESTING:

SNOWFALL

Is the mean snowfall in inches different in recent years than in previous years?

My null and alternative hypotheses:

$$H_{O}: \mu_{recent} - \mu_{nonrecent} = 0$$

$$H_{A}: \mu_{recent} - \mu_{nonrecent} \neq 0$$

```{r}
#Categorizing data
vt_data$category <- ifelse(vt_data$year %in% c(2015, 2016, 2020), "recent", "not")

#Running a t.test on the filtered data:
result <- t.test(snowfall ~ category, data = vt_data, conf.level = 0.95)
print(result)

#Extract confidence interval
conf_interval <- result$conf.int
print(paste('I am 95% confident the range', round(conf_interval[1], 2), 'and',  round(conf_interval[2], 2), 'contains the true population difference between recent and non recent years for average snowfall in inches on Mount Mansfield.'))

```

Is there a statistically significant relationship between snowfall in inches on year categories?

**Yes, p value is .0000000012 with a significance level of .05.** In this case I would reject the null hypothesis and this result is statistically significant at the .05 significance level. I have strong evidence to suggest that the observed difference between snowfall in recent years and non-recent years is unlikely to have occurred by random chance alone.

Some considerations for other factors could include wind speeds at the monitoring station, el nino/la nina years. Given this was over one monitor for a long duration, changes could have occurred to the monitor's position, measurement variability, and quality since snowfall is a quite sensitive variable.

A consideration for this analysis would include other variables such as wind speeds, el nino/la nina years, and other precipitation such as hail or ice. Given this was over one weather station for a longer duration, changes could have occurred to the monitor's position, measurement variability, and quality since snowfall can be very sensitive to measure.

TEMPERATURE

Is the mean temperature in fahrenheit different in recent years than in previous years?

My null and alternative hypotheses:

$$H_{O}: \mu_{recent} - \mu_{nonrecent} = 0$$

$$H_{A}: \mu_{recent} - \mu_{nonrecent} \neq 0$$

```{r}
#Categorizing data
vt_data$category <- ifelse(vt_data$year %in% c(2015, 2016, 2020), "recent", "not")

#Running a t.test on the filtered data:
result <- t.test(temp ~ category, data = vt_data, conf.level = 0.95)
print(result)

#Extract confidence interval
conf_interval <- result$conf.int
print(paste('I am 95% confident the range', round(conf_interval[1], 2), 'and',  round(conf_interval[2], 2), 'contains the true population difference between recent and non recent years for average temperature in F on Mount Mansfield.'))
```

Conclusion:

Fail to reject the null hypothesis!
In this case I would fail to reject the null hypothesis and this result is not statistically significant at the .05 significance level. I do not have strong evidence to suggest that the observed difference between temperature in recent years and non-recent years is unlikely to have occurred by random chance alone.

A consideration for this analysis could include other variables such as el nino/la nina years, relative humidity, and precipitation. Given this was over one weather station for a longer duration, changes could have occurred to the monitor's position, measurement variability, and quality since temperature can potentially be a sensitive variable to measure.


Linear Regression

```{r}
# I'm going to join my larger datasets together first:
vt_all_years_data <- merge(tidy_vt_snow_winter, tidy_vt_temps_winter, by = 'date')

#taking a look at the data:
vt_all_years_data <- vt_all_years_data %>% 
  select(-"Min.x", -"10%.x",-"30%.x", -"70%.x", -"90%.x", -"Max.x", -"Median (POR).x", -"year.x", -"month.x", -"Min.y",-"10%.y", -"30%.y",-"70%.y",-"90%.y",-"Max.y",-"Median (POR).y") %>% 
  rename(year = "year.y", month = "month.y")
vt_all_years_data
#aggregating all years of data for snowfall and temp:

#running a simple regression on snowfall and temperature variables for my sampling years
summary(lm(snowfall ~ temp, data = vt_all_years_data))
```
**Intercept: When the average temperature is 0 degrees Fahrenheit, the snowfall is, on average, 20.6 inches between 2001-2023.**

**Slope: For every 1 degree increase in temperature each day, the rate of snowfall decreases by .35.**

**R-Squared: With an r-squared value of .40, I can conclude that 40% of the snowfall data can be explained by temperature alone.








OTHER ANALYSES: These did not have any conclusions and required further learning/work to complete.

```{r}
#Tried a moving average: 
ma_window_size <- 7
ma_result <- rollmean(vt_data$snowfall, k = ma_window_size, fill = NA)
plot(vt_data$snowfall, type = "l", col = "blue", main = "Time Series with Moving Average")
lines(ma_result, col = "red", lty = 2)
legend("topright", legend = c("Original", "Moving Average"), col = c("blue", "red"), lty = 1:2)
```

```{r}
#Tried a time series decomposition:
test <- as_tsibble(tidy_vt_temps) 
 test %>% 
  model(classical_decomposition(snowfall, type = "additive")) %>% 
  components() %>% autoplot() + 
  labs(title = "Time Series Decomposition")
 
```

```{r}
#Wilcox Test 
result <- wilcox.test(vt_recent$snow_mean, vt_non_recent$snow_mean, alternative = "two.sided")

# Display the test result
print(result)
```
